\documentclass[12pt]{article}%
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.2cm, right=2.2cm]%
{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{changepage}
\usepackage{amssymb}
\usepackage{graphicx}%
\usepackage{array}
\usepackage{cite}
\usepackage{geometry} 
\usepackage{graphics}
\usepackage{caption}  
\usepackage{subfigure}
\usepackage{float}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage[toc,page]{appendix} 
\usepackage{hyperref}

\setcounter{MaxMatrixCols}{30}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

\begin{document}

\title{EE660: Facebook Comment Volume Prediction}
\author{Lin Huang, huanglin@usc.edu, Tianqi Wu, tianqi@usc.edu}
\date{\today}
\maketitle

A brief, informative description of your project. Include the problem, approach, and key results.
\tableofcontents
\section{Abstract}
Due to the rapid development of the Internet, there has been a huge expansion of the data in social networking for the last decade. It is of great significance for us to take advantage of the huge amount of data in order to model the behavior of online customers. In our project, we are aimed at taking advantage of the online customers' comments based on popular social company, Facebook's service ``Facebook Pages'' in order to model the behavior of the users. More specifically, our work is divided into 3 parts. First, we applied regression algorithms to predict comment volume prediction. Second, with multiple levels of popularity of post being our classification labels, various classification algorithms are utilized to classify the post. Finally, we also applied clustering algorithms in order to explore the relationship between the category of the post and the comment count.  Key results: 

\section{Problem Statement and Goals}
A brief description of the problem that you are trying to solve, and/or the goals you want to achieve. Clearly state the type of your problem (classification or regression) and which classes or variable you are trying to predict. Explain why it?s important or interesting, and why it?s not trivial.Normally difficulty can come from any of the following sources:(1) A physical model that?s inherently complicated and hard to abstract. (2) High dimensionality of feature space.(3) Sparsity.(4) Nonlinear behaviors.(5) Limited number of training samples.(6) Significant amounts of preprocessing required.

Due to the rapid development of the Internet, there has been a huge expansion of the data in social networking for the last decade. It is obvious that social networking has already become an essential part of our lives. It is of great significance for us to take advantage of the huge amount of data in order to model the behavior of online customers. We focus on one of the most famous online social service providers, Facebook's ``Facebook Pages'' service. Our goal is to model the pattern or behavior of online customers in terms of predicting the comment volume and the level of popularity over the posts on Facebook Pages and explore the relationships between comment volume and the category of the posts. Our problem consists of regression, classification, and clustering. 

prescient calculations on the remarks of most well known long range informal communication site, Facebook. We showed the customer remark patters, over the posts on Facebook Pages and expected that what number of remarks a post is depended upon to get in next H hrs. To automate the technique, we developed an item display containing the


\subsection{Learning-based approach}
Rule extraction from neural networks has been well researched, there are two basic approaches presented by author Andrews et al.\cite{andrews1995survey}, which are called decompositional and learning-based respectively. 
\\\\
The first method I adopt to extract rules from soft decision trees is learning-based approach. In this approach, the original algorithm is used in conjunction with another learning algorithm with inherent explanation capability. The basic idea is to use the first classifier to generate examples for a second learning algorithm that generates rules as outputs. Since the generalization behavior made by the output of the first algorithm is an indirect measure of the knowledge acquired by the algorithm, the second algorithm is utilized to generate rule sets that represent the generalization behavior\cite{barakat2004learning,123}.
\\\\
More specifically, there are 3 steps for this approach:
\begin{itemize}
\item A chosen dataset A is used to generate a Soft Decision Tree model.
\item The generated Soft Decision Tree is then used to predict labels for an extended dataset B. This step is to explore the
generalization behavior of the Soft Decision Tree. 
\item Dataset B or A+B are then used to train conventional hard decision tree to extract rules.
\end{itemize}


\subsection{Experimental results}     
In the experiments of the first method, we evaluate the extracted rules in terms of 3 criteria\cite{craven1999rule}:
\begin{itemize}
\item Accuracy: The ability of extracted representations to make accurate predictions on previously unseen cases.
\item Comprehensibility: The extent to which extracted representations are humanly comprehensible.
\item Fidelity: The extent to which extracted representations accurately model the learning algorithm from which they were extracted.
\end{itemize}
Accuracy is measured using confusion matrices and comparisons with DT\_gini,  DT\_ent, and  SDT are also made as shown in the following figures and table 1. 
\\\\
Fidelity of a rule set is the fraction of examples for which the rule set agrees with the trained soft decision tree\cite{andrews1995survey}, which is shown in table 1 as well.
 \\\\
Comprehensibility is measured using the syntactic complexity of generated trees as shown in table 2. More specifically, 2 aspects need to be taken into consideration: (1) the number of internal nodes in the tree, and (2) the number of symbols used in the splits of the tree\cite{craven1996extracting}. 
\\\\
5 datasets are tested, which are Banknotes data set(4 attributes), Pima Indians Diabetes data set(8 attributes), Breast Cancer data set(9 attributes), Twonorm data set(20 attributes), Spambase data set(57 attributes). Extracted rules and generated trees for Pima and Breast Cancer data sets are also shown in appendix B.

\subsubsection{Confusion matrices}

\subsubsection{Accuracy, fidelity, and comprehensibility}
\begin{table}[H]
\centering
\caption{Accuracy and fidelity}
\label{my-label}
\begin{tabular}{| l | l | l | l | l | l | l |}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Domain}} & \multicolumn{4}{c|}{Accuracy}                                                                                                 & \multicolumn{1}{||c|}{Fidelity}       \\ \cline{3-7} 
\multicolumn{2}{|c|}{}                        & \multicolumn{1}{c|}{DT\_ent} & \multicolumn{1}{c|}{DT\_gini} & \multicolumn{1}{c|}{SDT} & \multicolumn{1}{c|}{Learning-based} & \multicolumn{1}{||l|}{Learning-based} \\ \hline \hline
\multicolumn{2}{|c|}{Banknotes}  & \multicolumn{1}{c|}{97.8\%} &   \multicolumn{1}{c|}{97.1\%} &\multicolumn{1}{c|}{100\%}                          &\multicolumn{1}{c|}{98.9\%}                                     & \multicolumn{1}{|| c |}{98.9\%}                                      \\ \hline
\multicolumn{2}{|c|}{Pima}  &\multicolumn{1}{c|}{71.2\%}  & \multicolumn{1}{c|}{72.8\%}&\multicolumn{1}{c|}{74.3\%}&\multicolumn{1}{c|}{72.0\%} & \multicolumn{1}{|| c |}{92.2\%}                                      \\ \hline
\multicolumn{2}{|c|}{Breast}    &\multicolumn{1}{c|}{91.0\%}&\multicolumn{1}{c|}{93.2\%} & \multicolumn{1}{c|}{96.6\%} &\multicolumn{1}{c|}{96.6\%} & \multicolumn{1}{|| c |}{100\%}                                      \\ \hline
\multicolumn{2}{|c|}{Twonorm} &\multicolumn{1}{c|}{77.8\%}&\multicolumn{1}{c|}{76.6\%}& \multicolumn{1}{c|}{97.6\%} &\multicolumn{1}{c|}{83.5\%} &\multicolumn{1}{|| c |}{84.9\%}                                       \\ \hline
\multicolumn{2}{|c|}{Spambase} & \multicolumn{1}{c|}{87.7\%} &\multicolumn{1}{c|}{88.5\%}&\multicolumn{1}{c|}{92.8\%}& \multicolumn{1}{c|}{90.3\%}                              & \multicolumn{1}{|| c |}{92.4\%}                                      \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Comprehensibility}
\label{my-label}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Domain}} & \multicolumn{3}{c|}{\# Internal nodes} & \multicolumn{3}{||c|}{\# Symbols}     \\ \cline{3-8} 
\multicolumn{2}{|c|}{}                        & DT\_ent & DT\_gini  & Learning-based  &  \multicolumn{1}{|| c |}{DT\_ent}& DT\_gini & Learning-based \\ \hline \hline
\multicolumn{2}{|c|}{Banknotes}    &\multicolumn{1}{c|}{9}&\multicolumn{1}{c|}{11} &\multicolumn{1}{c|}{9}&\multicolumn{1}{|| c |}{9} & \multicolumn{1}{c|}{11} & \multicolumn{1}{c|}{9}   \\ \hline
\multicolumn{2}{|c|}{Pima}           &\multicolumn{1}{c|}{12} &\multicolumn{1}{c|}{14}&\multicolumn{1}{c|}{11}& \multicolumn{1}{|| c |}{12}  &\multicolumn{1}{c|}{14} &\multicolumn{1}{c|}{11} \\ \hline
\multicolumn{2}{|c|}{Breast}        & \multicolumn{1}{c|}{7}&\multicolumn{1}{c|}{8}&\multicolumn{1}{c|}{8}&  \multicolumn{1}{|| c |}{7}  &\multicolumn{1}{c|}{8} &\multicolumn{1}{c|}{8} \\ \hline
\multicolumn{2}{|c|}{Twonorm}      & \multicolumn{1}{c|}{15}&\multicolumn{1}{c|}{15}&\multicolumn{1}{c|}{15}&\multicolumn{1}{|| c |}{15} &\multicolumn{1}{c|}{15} &\multicolumn{1}{c|}{15}  \\ \hline
\multicolumn{2}{|c|}{Spambase}     & \multicolumn{1}{c|}{14}&\multicolumn{1}{c|}{15} & \multicolumn{1}{c|}{13}&\multicolumn{1}{|| c |}{14} &\multicolumn{1}{c|}{15}&\multicolumn{1}{c|}{13} \\ \hline
\end{tabular}
\end{table}


Based on the above confusion matrices and table 1, we can see that SDT performs better than conventional decision tree algorithms on the 5 data sets. Rules extracted from SDT using Learning-based approach also work better than conventional decision tree algorithms on every data set except for Pima data set. In terms of fidelity, it can be seen that Learning-based approach has done a great job of approximating corresponding SDT as well. 
\\\\
Table 2 shows that, for the 5 data sets, the comprehensibility of rules produced by the 3 methods is quite the same. Therefore, we argue that the rules extracted by Learning-based approach are as interpretable as the rules extracted by conventional decision tree algorithms.

\section{2nd and 3rd method of rules extraction: HALF and SUM}

\subsection{HALF and SUM}
For the 2 methods of rules extraction, the idea is to make use of the architecture of soft decision trees and the gating values at each node so that we are able to find the most probable path for each instance. Then, we assign the label which has more number of instances to corresponding path so that the rules are produced. 
\\\\
More specifically, in HALF, instances would only take nodes which have larger gating values. In other words,  instances would be directed through nodes with gating values greater than 0.5.
\\\\
For SUM, instances would choose path whose summation of gating values of its nodes is the biggest. 

\subsection{Experimental Results}
Following results show the confusion matrices of HALF and SUM methods for the 5 data sets we have experimented for Learning-based approach. Comparisons of accuracy and fidelity between all the 3 methods proposed are also made in table 3.

\subsubsection{Confusion matrices}


\subsubsection{Accuracy and fidelity}

\begin{table}[H]
\centering
\caption{Accuracy and fidelity}
\label{my-label}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Domain}} & \multicolumn{3}{c|}{Accuracy} & \multicolumn{3}{||c|}{Fidelity}     \\ \cline{3-8} 
\multicolumn{2}{|c|}{}                        & Learning-based & HALF  & SUM  &  \multicolumn{1}{|| c |}{Learning-based}& HALF & SUM \\ \hline \hline
\multicolumn{2}{|c|}{Banknotes}    &\multicolumn{1}{c|}{98.9\%}&\multicolumn{1}{c|}{99.8\%} &\multicolumn{1}{c|}{99.7\%}&\multicolumn{1}{|| c |}{98.9\%} & \multicolumn{1}{c|}{99.8\%} & \multicolumn{1}{c|}{99.6\%}   \\ \hline
\multicolumn{2}{|c|}{Pima}           &\multicolumn{1}{c|}{72.0\%} &\multicolumn{1}{c|}{77.4\%}&\multicolumn{1}{c|}{71.2\%}& \multicolumn{1}{|| c |}{92.2\%}  &\multicolumn{1}{c|}{87.5\%} &\multicolumn{1}{c|}{83.7\%} \\ \hline
\multicolumn{2}{|c|}{Breast}        & \multicolumn{1}{c|}{96.6\%}&\multicolumn{1}{c|}{97.4\%}&\multicolumn{1}{c|}{97.4\%}&  \multicolumn{1}{|| c |}{100\%}  &\multicolumn{1}{c|}{99.1\%} &\multicolumn{1}{c|}{98.3\%} \\ \hline
\multicolumn{2}{|c|}{Twonorm}      & \multicolumn{1}{c|}{83.5\%}&\multicolumn{1}{c|}{97.2\%}&\multicolumn{1}{c|}{97.2\%}&\multicolumn{1}{|| c |}{84.9\%} &\multicolumn{1}{c|}{99.2\%} &\multicolumn{1}{c|}{99.2\%}  \\ \hline
\multicolumn{2}{|c|}{Spambase}     & \multicolumn{1}{c|}{90.3\%}&\multicolumn{1}{c|}{92.6\%} & \multicolumn{1}{c|}{91.7\%}&\multicolumn{1}{|| c |}{92.4\%} &\multicolumn{1}{c|}{98.4\%}&\multicolumn{1}{c|}{97.2\%} \\ \hline
\end{tabular}
\end{table}


Based on the above results, we can conclude that HALF performs better than Learning-based approach and conventional hard decision trees in terms of accuracy on each data set while SUM is better on each data set except for pima data set. Moreover, compared with the fidelity of Learning-based approach, HALF and SUM also generally provide closer approximations to SDT. Therefore, it seems natural to reach the conclusion that HALF and SUM algorithms of rules extraction work better than Learning-based approach on the 5 data sets. 
\\\\
However, since soft decision trees are multivariate decision trees, which means it takes advantage of linear combination of all the attributes to split the input space instead of one attribute. Each decision node divides the input space into 2 spaces with a hyperplane of arbitrary orientation. Thus, the antecedents (if-clause) of the propositional 'If-Then' rules we extract actually consist of linear combination of attributes. We can take one rule I have extracted using HALF and SUM algorithms for breast cancer as an example. As shown in table 4 and 5, the rules are not quite comprehensible to humans due to the linear combination of attributes.
\\\\
Hence, in order to take advantage of the better performance of these 2 methods and make the rules more interpretable to humans, measures should be taken to make the "multivariate rules"(Rules which include linear combination of attributes) univariate. One idea I have is presented in the next section. 



\subsection{Univariate Rules}
According to the above analysis, even if we can have a better performance in terms of accuracy and fidelity by using HALF and SUM algorithms, the lack of comprehensibility still impedes us from applying these 2 methods to practical problems. Thus, in order to solve this problem, I have come up with one idea which combines fuzzy entropy mentioned in \cite{kumar2016ensemble} and soft decision tree. In \cite{kumar2016ensemble}, the author utilizes the concept of fuzzy set theory to build decision trees and the building process is similar to conventional decision trees:
\\\\
More specifically, assume that we have a input space $\emph{\textbf X}$ and we are at node $m$. The paper first uses membership functions to calculate membership values of each instance with respect to one attribute $A_i$ to the left and right child nodes of $m$, which are $m^L$ and $m^R$ respectively. Then, they make use of the membership values to compute the fuzzy entropy of input space $\emph{\textbf X}$ at node $m$, $m^L$, and $m^R$. After acquiring fuzzy entropy, they can find fuzzy information gain for attribute $A_i$. The information gains of the other attributes is calculated in the same way. Then, we can choose the attribute which has the largest information gain as the good splitting attribute at node $m$. The process is recursively repeated on the obtained child nodes from the parent node, to build the decision tree.
\\\\
According to the building process described above, if we view probabilities in soft decision trees as membership values to some extent. Then we can also apply the above process to soft decision trees. Using the probabilities to calculate the entropy and further compute the information gain in order to choose the best splitting attribute. After this process, we can get rid of linear combination of all the attributes as our split. Moreover, since we are able to have single attribute at each node, HALF or SUM algorithms of rules extraction can give us the univariate rules which are comprehensible to humans. More details of the building process is presented as follows.
\\\\
Assume we are at root node $m$. Let $\emph{\textbf X}$ be the input space, $\emph{\textbf x}_n$ represent the $n$th instance, $A_i$ be the $i$th attribute. The instance is composed of d attribute values, $\emph{\textbf x}_n = [x_{n1},x_{n2},...,x_{nd}]^T$. We still use sigmoid function as our gating function. So the gating values of $\emph{\textbf x}_n$ for attribute $A_i$ is defined as:
\emph {\[ g_m(x_{ni})=\frac {1}{1+e^{-(w_m^ix_{ni}+w_{m0}^i)}}\]}$F_m^L(\emph{\textbf x}_n)$ and $F_m^R(\emph{\textbf x}_n)$ represent the output of its left and right children. The response of the tree can be calculated as follows:
\emph {\[ F_m(\textbf x_n)=F_m^L(\textbf x_n)g_m(x_{ni})+F_m^R(\textbf x_n)(1-g_m(x_{ni}))\]}We construct the error function using the response $F_m(\emph{\textbf x}_n)$ and the true label of instance $\emph{\textbf x}_n$, which is $y_n$. The error function is cross-entropy for classification and square loss for regression:
\begin{equation*}
        E = \begin{cases}
                        \text{$\sum_{n=1}^{|\emph{\textbf X}|} (F_m(\emph{\textbf x}_n)-y_n)^2$}  & Regression\\
                        \text{$\sum_{n=1}^{|\emph{\textbf X}|} F_m(\emph{\textbf x}_n) log y_n + (1-F_m(\emph{\textbf x}_n)) log(1-y_n) $} &Classification
                    \end{cases}
\end{equation*}Then we utilize optimization method based on gradient-descent which is introduced in \cite{irsoy2012soft} to optimize the parameters.The above process is also repeated for the rest $d-1$ attributes. Since we have gotten the optimized gating values of each instance with respect to every attribute, we can further make use of the concept of fuzzy entropy to choose the best splitting attribute. The basic idea is to view the gating values as membership values. From above definition, the membership value of $i$th attribute to the left child node for one given instance should be $\emph g_m(x_{ni})$. The corresponding membership value to the right child node is 1$-$$\emph g_m(x_{ni})$. For convenience, in the rest part of this section, we use $\emph g_m^L(x_{ni})$ and $\emph g_m^R(x_{ni})$ to denote the left and right membership values at root node $m$ respectively. 
\\\\
Then, we can define soft entropy of data set $\emph{\textbf X}$ for attribute $A_i$ at parent nodes as follows:
\emph {\[ Ent_m(\textbf X)=\sum_{k=1}^{l} -(P_k,\textbf X)log_2(P_k,\textbf X)\]}where $\emph (P_k,\emph{\textbf X})=\frac{\sum_{x_{ni}\in c_k}(g_m^L(x_{ni})+g_m^R(x_{ni}))}{N_m}$, $N_m=\sum_{n=1}^{|\emph{\textbf X}|}(g_m^L(x_{ni})+g_m^R(x_{ni}))$, $c= [c_1,c_2,...,c_l]$ is labels of $\emph{\textbf X}$. $\emph (P_k,\emph{\textbf X})$ represents the probability distribution of instances in $\emph{\textbf X}$ that belongs to class $c_k$.
\\\\
Similarly, the soft entropy of the left and right child nodes are defined as following:
\emph {\[ Ent_m(\textbf X^L)=\sum_{k=1}^{l} -(P_k^L,\textbf X^L)log_2(P_k^L,\textbf X^L)\]}
\emph {\[ Ent_m(\textbf X^R)=\sum_{k=1}^{l} -(P_k^R,\textbf X^R)log_2(P_k^R,\textbf X^R)\]}where $\emph (P_k^L,\emph{\textbf X}^L)=\frac{\sum_{x_{ni}\in c_k}(g_m^L(x_{ni}))}{N_m^L}$, $\emph (P_k^R,\emph{\textbf X}^R)=\frac{\sum_{x_{ni}\in c_k}(g_m^R(x_{ni}))}{N_m^R}$, $N_m^L=\sum_{n=1}^{|\emph{\textbf X}|}(g_m^L(x_{ni}))$, $N_m^R=\sum_{n=1}^{|\emph{\textbf X}|}(g_m^R(x_{ni}))$. $\emph (P_k^L,\emph{\textbf X}^L)$ and $\emph (P_k^R,\emph{\textbf X}^R)$ represent the probability distribution of instances in $\emph{\textbf X}^L$ and $\emph{\textbf X}^R$ that belongs to class $c_k$ respectively.
\\\\
After calculating the soft entropy, similar to conventional decision tree algorithm, we can further compute the corresponding information gain or we call soft information gain for attribute $A_i$ at the root node m:
\emph {\[Gain_m(A_i,\textbf X)=Ent_m(\textbf X)-(\frac{N_m^L}{N_m}Ent_m(\textbf X^L)+\frac{N_m^R}{N_m}Ent_m(\textbf X^R))\]}The same process is applied to the rest $d-1$ attributes in $\emph{\textbf X}$, then we choose the attribute with the biggest soft information gain as the best splitting attribute at the root node $m$. The above process is recursively repeated on the obtained child nodes from the root node, to build the univariate soft decision tree.
\\\\
After transforming the original multivariate soft decision trees into the univariate soft decision trees, we can apply the HALF or SUM algorithms to extract univariate rules from such trees. In this way, we should have a good chance to extract interpretable univariate rules from soft decision trees and preserve the higher performance of HALF or SUM algorithms.

\section{Conclusion}
In this semester of directed research, I have become familiar with the concepts of classification, hard decision trees, fuzzy logics, fuzzy decision trees, soft decision trees, and rules extraction, etc. After acquiring the fundamental knowledge, I have started to consider methods of extracting rules from soft decision trees. Learning-based approach, HALF, and SUM algorithms have been come up with and experimental results on 5 common data sets have shown that the proposed Learning-based approach has better performance in terms of accuracy, fidelity, and comprehensibility than conventional decision trees and HALF and SUM algorithms is better than Learning-based approach in terms of accuracy and fidelity. However, due to the lack of comprehensibility of HALF and SUM algorithms, soft decision tree is combined with the concept of fuzzy entropy in order to generate univariate rules. 

\clearpage
\bibliographystyle{ieeetr}
\bibliography{Reference}

\clearpage
\begin{appendix}  
\section{Codes}
\href{https://github.com/LinHuang17/Rule_Extraction_Soft-Decision-Tree}{Github Codes link} \\
\url{https://github.com/LinHuang17/Rule_Extraction_Soft-Decision-Tree}
\section{Generated rules and trees using Learning-based approach }  


\end{appendix}  

\end{document}8










